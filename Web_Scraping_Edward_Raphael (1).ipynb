{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEJYrRqhCUkR",
        "outputId": "bbc13f44-8c2b-40a8-d385-ea8ea4886f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping Complete\n"
          ]
        }
      ],
      "source": [
        "#COIN MARKET\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "url = \"https://coinmarketcap.com/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "table = soup.find('table', {'class': 'sc-7b3ac367-3 etbcea cmc-table'})\n",
        "\n",
        "with open('Crypto.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['No.','Name', 'Price', '1h%', '24h%', '7d%', 'Market Cap', 'Volume(24h)', 'Circulating Supply'])\n",
        "\n",
        "    for row in table.find_all('tr')[1:]:\n",
        "        columns = row.find_all('td')\n",
        "\n",
        "        if len(columns) >= 9:\n",
        "            Number = columns[1].text.strip()\n",
        "            Name = columns[2].text.strip()\n",
        "            Price = columns[3].text.strip()\n",
        "            Hour = columns[4].text.strip()\n",
        "            Day = columns[5].text.strip()\n",
        "            Week = columns[6].text.strip()\n",
        "            MarketCap = columns[7].text.strip()\n",
        "            Volume = columns[8].text.strip()\n",
        "            CirculatingSupply = columns[9].text.strip()\n",
        "\n",
        "            writer.writerow([Number, Name, Price, Hour, Day, Week, MarketCap, Volume, CirculatingSupply])\n",
        "\n",
        "print(\"Scraping Complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "url = 'https://www.theguardian.com/football/premierleague/table'\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "table = soup.find('table', {'class': 'table table--football table--league-table table--responsive-font table--striped'})\n",
        "\n",
        "with open('PremierLeagueDataset.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['P','Team','GP',\t'W', 'D', 'L', 'F', 'A', 'GD', 'Pts', 'Form'])\n",
        "\n",
        "    for row in table.find_all('tr')[1:]:\n",
        "        columns = row.find_all('td')\n",
        "\n",
        "        if len(columns) >= 11:\n",
        "            P = columns[0].text.strip()\n",
        "            Team = columns[1].text.strip()\n",
        "            GP = columns[2].text.strip()\n",
        "            W = columns[3].text.strip()\n",
        "            D = columns[4].text.strip()\n",
        "            L = columns[5].text.strip()\n",
        "            F = columns[6].text.strip()\n",
        "            A = columns[7].text.strip()\n",
        "            GD = columns[8].text.strip()\n",
        "            PTS = columns[9].text.strip()\n",
        "            Form = columns[10].text.strip()\n",
        "\n",
        "            writer.writerow([P, Team, GP, W, D, L, F, A, GD, PTS, Form])\n",
        "\n",
        "print(\"Scraping complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC47ZWnDV-cx",
        "outputId": "6ef50659-128e-402b-a858-c5b4c72f65b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to 'PremierLeagueDataset.csv'.\n"
          ]
        }
      ]
    }
  ]
}